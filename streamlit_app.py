# -*- coding: utf-8 -*-
"""streamlit_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WfgCxlCic3W-ShHW5DjLhRPo04ly64Ji
"""

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mainapp.py
# import streamlit as st
# from langchain.chat_models import ChatOpenAI
# import os
# from langchain.schema import (
#     SystemMessage,
#     HumanMessage,
#     AIMessage,
# )
# from datasets import load_dataset
# import pinecone  # Assuming you have pinecone-client installed
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.vectorstores import Pinecone
# 
# # Set API keys as environment variables
# os.environ["OPENAI_API_KEY"] = os.getenv("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")
# os.environ["PINECONE_API_KEY"] = os.getenv("25555acb-fab7-461a-a9ec-ec8427e42543")
# 
# # Initialize OpenAI chat model
# chat = ChatOpenAI(
#     openai_api_key=os.environ["OPENAI_API_KEY"],
#     model="gpt-3.5-turbo"
# )
# 
# # Initialize Pinecone connection
# pinecone.init(
#     api_key=os.environ["PINECONE_API_KEY"],
#     environment="gcp-starter"
# )
# 
# # Define index and embedding model names
# index_name = "llama-2-rag"
# embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
# 
# # Create/check Pinecone index
# if index_name not in pinecone.list_indexes():
#     pinecone.create_index(
#         index_name,
#         dimension=1536,  # Dimensionality of ada 002
#         metric="cosine"
#     )
# 
#     while not pinecone.describe_index(index_name).status["ready"]:
#         time.sleep(1)
# 
# index = pinecone.Index(index_name)
# 
# # Helper function to load dataset
# def load_data():
#     dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked", split="train")
#     return dataset.to_pandas()
# 
# # Helper function to embed text
# def embed_text(texts):
#     return embed_model.embed_documents(texts)
# 
# # Helper function to create augmented prompt
# def create_augmented_prompt(query, data, vectorstore):
#     results = vectorstore.similarity_search(query, k=3)
#     source_knowledge = "\n".join([x["chunk"] for x in results])
#     return f"""Using the contexts below, answer the query.
# 
# Contexts:
# {source_knowledge}
# 
# Query: {query}"""
# 
# 
# # Streamlit app setup
# st.title("AI Chatbot with Pinecone Knowledge Base")
# 
# user_query = st.text_input("Ask me anything!")
# 
# # Load data and initialize vectorstore on first run
# if "data" not in st.session_state:
#     st.session_state["data"] = load_data()
#     st.session_state["vectorstore"] = Pinecone(
#         index, embed_model.embed_query, "text"
#     )
# 
# # Get vectorstore and data from session state
# vectorstore = st.session_state["vectorstore"]
# data = st.session_state["data"]
# 
# # Check if user entered a query
# if user_query:
# 
#     # Initialize conversation with system message
#     messages = [
#         SystemMessage(content="You are a helpful assistant."),
#         HumanMessage(content=user_query),
#     ]
# 
#     # Get initial response from OpenAI
#     res = chat(messages)
#     messages.append(res)
# 
#     # Create augmented prompt and get response
#     augmented_prompt = create_augmented_prompt(
#         user_query, data, vectorstore
#     )
#     messages.append(HumanMessage(content=augmented_prompt))
#     res = chat(messages)
# 
#     # Display conversation history and AI response
#     for message in messages:
#         st.write(message.content)
# 
#     st.success(res.content)
#

# Create a package.json file with default values
!echo '{"name": "colab", "version": "1.0.0", "main": "index.js", "scripts": {"start": "echo \"Error: no start script specified\" && exit 1"}, "keywords": [], "author": "", "license": "ISC"}' > /content/package.json

# Install localtunnel
!npm install localtunnel

!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8000 & curl ipv4.icanhazip.com

